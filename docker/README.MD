

A complete end-to-end **real-time + batch clickstream analytics pipeline** built using:

* Apache Kafka â€“ real-time event ingestion
* Apache Spark Structured Streaming â€“ real-time processing
* Hadoop HDFS â€“ distributed storage
* Spark Batch ETL â€“ daily aggregations
* Apache Superset â€“ visualization dashboard
* Docker Compose â€“ containerized multi-service environment

This project simulates an **e-commerce clickstream pipeline** that ingests live events, processes them in real time, stores them in HDFS, and visualizes insights in dashboards.

---

## Architecture Overview

User Clicks â†’ Kafka Producer â†’ Kafka Topic â†’ Spark Streaming
â†“                                       â†“
Superset Dashboard      HDFS (Parquet Storage)
â†“
Spark Batch ETL â†’ CSV
â†“
Dashboard Visuals

---

## ðŸ“¦ech Stack

| Component       | Purpose                                           |
| --------------- | ------------------------------------------------- |
| Kafka           | Real-time ingestion of click events               |
| Spark Streaming | Processes incoming clickstream                    |
| HDFS (Hadoop)   | Distributed storage (Parquet files + checkpoints) |
| Spark Batch Job | Converts Parquet â†’ CSV for analytics              |
| Superset        | Dashboard tool                                    |
| Docker          | Runs all services in isolated containers          |

---

## ðŸ—‚ Project Structure

clickstream_project/
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ produce_clicks.py
â”‚
â”œâ”€â”€ streaming/
â”‚   â””â”€â”€ streaming_job.py
â”‚
â”œâ”€â”€ batch/
â”‚   â””â”€â”€ batch_etl.py
â”‚
â”œâ”€â”€ convert_parquet_to_csv.py
â””â”€â”€ README.md

---

# How to Run the Project (Commands Only)

### Start the entire environment

```
docker compose up -d
```

### Start Kafka clickstream producer

```
docker exec -it spark-master bash -lc "python /shared/producer/produce_clicks.py"
```

### Start Spark Streaming job

```
docker exec -it spark-master bash -lc "/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 --master spark://spark-master:7077 /shared/streaming/streaming_job.py"
```

### Start Spark Batch ETL job

```
docker exec -it spark-master bash -lc "/spark/bin/spark-submit --master spark://spark-master:7077 /shared/batch/batch_etl.py"
```

### Convert Parquet â†’ CSV (if needed)

```
docker exec -it spark-master bash -lc "/spark/bin/spark-submit /shared/convert_parquet_to_csv.py"
```

---

# Dashboard (Superset)

### Access Superset UI:

```
http://localhost:8088
```

### Login:

```
username: admin
password: admin
```

### Steps to build dashboard:

1. Upload processed CSV file
2. Create a dataset
3. Build charts:

   * Hourly active users
   * Page views over time
   * Most viewed product pages
   * Session length patterns
4. Combine everything into a final dashboard

---

# What This Project Demonstrates

âœ” Real-time streaming analytics
âœ” Kafkaâ€“Spark integration
âœ” Distributed storage with HDFS
âœ” Batch processing workflow
âœ” Visual analytics with Superset
âœ” Docker-based big data environment

---

# Final Dashboard Insights

The dashboard shows:

* Active users per hour
* Real-time click rate
* Most viewed pages
* Geographic/device distribution
* Conversion metrics
* Session trends

---



